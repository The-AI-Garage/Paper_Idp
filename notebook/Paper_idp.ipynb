{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5397b459-fde5-4299-a32d-1a644047d273",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SetUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8c073f63-7a6e-4bee-b453-156a1e8fad43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (24.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install LangChain --quiet\n",
    "%pip install langchain-community --quiet\n",
    "%pip install faiss-cpu --quiet\n",
    "%pip install -U langchain-aws --quiet\n",
    "%pip install amazon-textract-caller --quiet\n",
    "%pip install amazon-textract-textractor --quiet\n",
    "%pip install anthropic --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40f63ef3-3f65-4f70-ac18-1207937d26fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import boto3\n",
    "\n",
    "from langchain_community.document_loaders import AmazonTextractPDFLoader\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain.llms import Bedrock\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.example_selectors import (\n",
    "    MaxMarginalRelevanceExampleSelector,\n",
    "    SemanticSimilarityExampleSelector,\n",
    ")\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09575264-e194-460e-a7fe-58d59f86f8d4",
   "metadata": {},
   "source": [
    "## Prompt engineering (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12f39a7f-9d95-45a2-a6ff-ed2865bcdcc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# examples with the abstract of the papers\n",
    "examples = [\n",
    "    {\n",
    "    \"Text\": \"The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-oneout cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 crossvalidation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.\",\n",
    "    \"Output\": \"General ML\",\n",
    "    },\n",
    "    {\n",
    "    \"Text\": \"As one of the most successful approaches to building recommender systems,collaborative filtering(CF) uses the known preferences of a group of users to make recommendations or predictions of the unknown preferences for other users. In this paper, we first introduce CF tasks and their main challenges, such as data sparsity, scalability, synonymy, gray sheep, shilling attacks, privacy protection, etc., and their possible solutions. We then present three main categories of CF techniques: memory-based, modelbased, and hybrid CF algorithms (that combine CF with other recommendation techniques), with examples for representative algorithms of each category, and analysis of their predictive performance and their ability to address the challenges. From basic techniques to the state-of-the-art, we attempt to present a comprehensive survey for CF techniques, which can be served as a roadmap for research and practice in this area.\",\n",
    "    \"Output\": \"Recommenders\", \n",
    "    },\n",
    "    { \n",
    "    \"Text\": \"This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don’t worry if you get stuck at some point along the way—just go back and reread the previous section, and try writing down and working through some examples. And if you’re still stuck, we’re happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here.\",\n",
    "    \"Output\": \"Neural Networks\",\n",
    "    },\n",
    "    {\n",
    "    \"Text\": \"Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error onthe ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. training error (%) 10 Jian Sun 20 56-layer 20-layer 0 test error (%) 10 0 56-layer 20-layer 0 6 5 4 3 2 1 0 iter. (1e4) 1 2 3 4 iter. (1e4) 5 6 Figure 1. Training error (left) and test error (right) on CIFAR-10 with 20-layer and 56-layer “plain” networks. The deeper network has higher training error, and thus test error. Similar phenomena on ImageNet is presented in Fig. 4. greatly benefited from very deep models. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.\",\n",
    "    \"Output\": \"Object Detection\", \n",
    "    },\n",
    "    {\n",
    "    \"Text\": \"Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation\",\n",
    "    \"Output\": \"NLP\",\n",
    "    },  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6486ed58-2aed-4e95-a6ee-71d98fa2df0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_claude = \"\"\"\n",
    "\n",
    "<instructions>\n",
    "You are a Scientific paper system. \n",
    "Please do the following:\n",
    "1.  Given a list of classes, \n",
    "    classify the document into one of the classes into the <classes></classes> tags.\n",
    "    Think your answer with the following reasoning: \n",
    "    First, check the examples showed into the <example></example> tags. \n",
    "    Second, list CLUES like: \n",
    "    What is the paper about?,\n",
    "    What kind of issue the paper is addressing?, \n",
    "    What kind of machine learning field this paper is related? and why?.\n",
    "    Before reply add your reasoning into the <thinking></thinking> tags.\n",
    "    Skip any preamble text and provide your final answer \n",
    "    just replay with the class name within <label></label> tags.\n",
    "</instructions>\n",
    "\n",
    "<classes>General ML, Recommenders, Neural Networks, Object Detection, NLP</classes>\n",
    "\n",
    "here are some examples of text documents with their expected output: <example>\"\"\" \n",
    "\n",
    "suffix_template=\"\"\"\n",
    "</example>\n",
    "<document>{doc_text}</document>\n",
    "<label></label>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74c57d7e-adf6-4b26-b0bb-a7b60936448a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"Text\", \"Output\"],\n",
    "    template=\"Text: {Text}\\nOutput: {Output}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b7e02d98-500a-4d12-bf97-cb55313d0ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "example_selector = MaxMarginalRelevanceExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    BedrockEmbeddings(model_id= 'amazon.titan-embed-text-v1'),\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS,\n",
    "    # The number of examples to produce.\n",
    "    k=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9b54d2f9-d3b4-4f86-84e0-dd495b466407",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Text: The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-oneout cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 crossvalidation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.\n",
      "Output: General ML\n"
     ]
    }
   ],
   "source": [
    "text_examp = \"The Machine Learning field evolved from the broad field of Artificial Intelligence, which aims to mimic intelligent abilities of humans by machines. In the field of Machine Learning one considers the important question of how to make machines able to “learn”. Learning in this context is understood as inductive inference, where one observes examples that represent incomplete information about some “statistical phenomenon”. In unsupervised learning one typically tries to uncover hidden regularities (e.g. clusters) or to detect anomalies in the data (for instance some unusual machine function or a network intrusion). In supervised learning, there is a label associated with each example. It is supposed to be the answer to a question about the example. If the label is discrete, then the task is called classification problem– otherwise, for realvalued labels we speak of a regression problem. Based on these examples (including the labels), one is particularly interested to predict the answer for other cases before they are explicitly observed. Hence, learning is not only a question of remembering but also of generalization to unseen cases\"\n",
    "selected_examples = example_selector.select_examples({\"Text\": text_examp})\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d3d42d80-1158-4efa-a293-f6d2fc2b9404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mmr_prompt = FewShotPromptTemplate(\n",
    "    # We provide an ExampleSelector instead of examples.\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prompt_claude, #A prompt template string to put before the examples.\n",
    "    suffix=suffix_template, #A prompt template string to put after the examples.\n",
    "    input_variables=[\"doc_text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c90adabe-5d65-4f82-9287-cc8a424fd17c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<instructions>\n",
      "You are a Scientific paper system. \n",
      "Please do the following:\n",
      "1.  Given a list of classes, \n",
      "    classify the document into one of the classes into the <classes></classes> tags.\n",
      "    Think your answer with the following reasoning: \n",
      "    First, check the examples showed into the <example></example> tags. \n",
      "    Second, list CLUES like: \n",
      "    What is the paper about?,\n",
      "    What kind of issue the paper is addressing?, \n",
      "    What kind of machine learning field this paper is related? and why?.\n",
      "    Before reply add your reasoning into the <thinking></thinking> tags.\n",
      "    Skip any preamble text and provide your final answer \n",
      "    just replay with the class name within <label></label> tags.\n",
      "</instructions>\n",
      "\n",
      "<classes>General ML, Recommenders, Neural Networks, Object Detection, NLP</classes>\n",
      "\n",
      "here are some examples of text documents with their expected output: <example>\n",
      "\n",
      "Text: The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-oneout cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 crossvalidation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.\n",
      "Output: General ML\n",
      "\n",
      "\n",
      "</example>\n",
      "<document>The Machine Learning field evolved from the broad field of Artificial Intelligence, which aims to mimic intelligent abilities of humans by machines. In the field of Machine Learning one considers the important question of how to make machines able to “learn”. Learning in this context is understood as inductive inference, where one observes examples that represent incomplete information about some “statistical phenomenon”. In unsupervised learning one typically tries to uncover hidden regularities (e.g. clusters) or to detect anomalies in the data (for instance some unusual machine function or a network intrusion). In supervised learning, there is a label associated with each example. It is supposed to be the answer to a question about the example. If the label is discrete, then the task is called classification problem– otherwise, for realvalued labels we speak of a regression problem. Based on these examples (including the labels), one is particularly interested to predict the answer for other cases before they are explicitly observed. Hence, learning is not only a question of remembering but also of generalization to unseen cases</document>\n",
      "<label></label>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(mmr_prompt.format(doc_text=text_examp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ca84acc-2dd2-4559-904d-b2ea6ab28e3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load paper from s3\n",
    "# Initialize S3 client and set RoleArn, region name, and bucket name\n",
    "s3 = boto3.client(\"s3\")\n",
    "roleArn = 'arn:aws:iam::013987100154:role/textractTest'\n",
    "region_name = 'us-east-1'\n",
    "bucket_name = 'llm-showcase'\n",
    "\n",
    "# to hold all docs in bucket\n",
    "docs_list = []\n",
    "\n",
    "# loop through docs in bucket, get names of all docs\n",
    "s3_resource = boto3.resource(\"s3\")\n",
    "bucket = s3_resource.Bucket(bucket_name)\n",
    "for bucket_object in bucket.objects.filter(Prefix='papers/', Delimiter='/'):\n",
    "    if bucket_object.key.endswith('.pdf'):\n",
    "        docs_list.append(bucket_object.key)\n",
    "\n",
    "        \n",
    "file_path = f\"s3://llm-showcase/\" + docs_list[0]\n",
    "loader = AmazonTextractPDFLoader(file_path)\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f25e41f3-bace-426b-87a4-38f20b4a2ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cantidad de paginas 6\n"
     ]
    }
   ],
   "source": [
    "print(\"cantidad de paginas {}\".format(len(document)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b794c7b-cd83-4b8c-8750-522bab4a52a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### document summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1f03f8e9-b642-4592-9cfc-5cbf452ce5ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#summarize paper with map reduce component\n",
    "bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "bedrock_llm = BedrockChat(client=bedrock, model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\")\n",
    "\n",
    "summary_chain = load_summarize_chain(llm=bedrock_llm, chain_type='map_reduce')\n",
    "output = summary_chain.invoke(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4fd9a63f-0fac-4408-8646-8d31e641986e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_documents': [Document(page_content='A Brief Introduction into Machine Learning\\n\\n\\nGunnar Rätsch\\n\\n\\nFriedrich Miescher Laboratory of the Max Planck Society,\\n\\n\\nSpemannstraße 37, 72076 Tübingen, Germany\\n\\n\\nttp://www.tuebingen.mpg.de/-raetsch\\n\\n\\n1 Introduction\\n\\n\\nThe Machine Learning field evolved from the broad field of Artificial Intelligence,\\n\\n\\nwhich aims to mimic intelligent abilities of humans by machines. In the field of Ma-\\n\\n\\nchine Learning one considers the important question of how to make machines able\\n\\n\\nto \"learn\". Learning in this context is understood as inductive inference, where one\\n\\n\\nobserves examples that represent incomplete information about some \"statistical phe-\\n\\n\\nnomenon\". In unsupervised learning one typically tries to uncover hidden regularities\\n\\n\\n(e.g. clusters) or to detect anomalies in the data (for instance some unusual machine\\n\\n\\nfunction or a network intrusion). In supervised learning, there is a label associated\\n\\n\\nwith each example. It is supposed to be the answer to a question about the example. If\\n\\n\\nthe label is discrete, then the task is called classification problem - otherwise, for real-\\n\\n\\nvalued labels we speak of a regression problem. Based on these examples (including\\n\\n\\nthe labels), one is particularly interested to predict the answer for other cases before\\n\\n\\nthey are explicitly observed. Hence, learning is not only a question of remembering\\n\\n\\nbut also of generalization to unseen cases.\\n\\n\\n2 Supervised Classification\\n\\n\\nAn important task in Machine Learning is classification, also referred to as pattern\\n\\n\\nrecognition, where one attempts to build algorithms capable of automatically con-\\n\\n\\nstructing methods for distinguishing between different exemplars, based on their dif-\\n\\n\\nferentiating patterns.\\n\\n\\nWatanabe [1985] described a pattern as \"the opposite of chaos; it is an entity,\\n\\n\\nvaguely defined, that could be given a name.\" Examples of patterns are human faces,\\n\\n\\ntext documents, handwritten letters or digits, EEG signals, and the DNA sequences that\\n\\n\\nmay cause a certain disease. More formally, the goal of a (supervised) classification\\n\\n\\ntask is to find a functional mapping between the input data X, describing the input\\n\\n\\npattern, to a class label Y (e.g. or +1), such that Y = f(X). The construction of\\n\\n\\nthe mapping is based on so-called training data supplied to the classification algorithm.\\n\\n\\nThe aim is to accurately predict the correct label on unseen data.\\n\\n\\nA pattern (also: \"example\") is described by its features. These are the characteris-\\n\\n\\ntics of the examples for a given problem. For instance, in a face recognition task some\\n\\n\\nfeatures could be the color of the eyes or the distance between the eyes. Thus, the input\\n\\n\\n1\\n', metadata={'source': 's3://llm-showcase/papers/105-machine-learning-paper.pdf', 'page': 1}),\n",
       "  Document(page_content='to a pattern recognition task can be viewed as a two-dimensional matrix, whose axes\\n\\n\\nare the examples and the features.\\n\\n\\nPattern classification tasks are often divided into several sub-tasks:\\n\\n\\n1. Data collection and representation.\\n\\n\\n2. Feature selection and/or feature reduction.\\n\\n\\n3. Classification.\\n\\n\\nData collection and representation are mostly problem-specific. Therefore it is difficult\\n\\n\\nto give general statements about this step of the process. In broad terms, one should\\n\\n\\ntry to find invariant features, that describe the differences in classes as best as possible.\\n\\n\\nFeature selection and feature reduction attempt to reduce the dimensionality (i.e.\\n\\n\\nthe number of features) for the remaining steps of the task. Finally, the classification\\n\\n\\nphase of the process finds the actual mapping between patterns and labels (or targets).\\n\\n\\nIn many applications the second step is not essential or is implicitly performed in the\\n\\n\\nthird step.\\n\\n\\n3\\n\\n\\nClassification Algorithms\\n\\n\\nAlthough Machine Learning is a relatively young field of research, there exist more\\n\\n\\nlearning algorithms than I can mention in this introduction. I chose to describe six\\n\\n\\nmethods that I am frequently using when solving data analysis tasks (usually classifi-\\n\\n\\ncation). The first four methods are traditional techniques that have been widely used\\n\\n\\nin the past and work reasonably well when analyzing low dimensional data sets with\\n\\n\\nnot too few labeled training examples. In the second part I will briefly outline two\\n\\n\\nmethods (Support Vector Machines & Boosting) that have received a lot of attention in\\n\\n\\nthe Machine Learning community recently. They are able to solve high-dimensional\\n\\n\\nproblems with very few examples (e.g. fifty) quite accurately and also work efficiently\\n\\n\\nwhen examples are abundant (for instance several hundred thousands of examples).\\n\\n\\n3.1 Traditional Techniques\\n\\n\\nk-Nearest Neighbor Classification Arguably the simplest method is the k-Nearest\\n\\n\\nNeighbor classifier [Cover and Hart, 1967]. Here the k points of the training data\\n\\n\\nclosest to the test point are found, and a label is given to the test point by a majority\\n\\n\\nvote between the k points. This method is highly intuitive and attains - given its\\n\\n\\nsimplicity - remarkably low classification errors, but it is computationally expensive\\n\\n\\nand requires a large memory to store the training data.\\n\\n\\nLinear Discriminant Analysis computes a hyperplane in the input space that min-\\n\\n\\nimizes the within-class variance and maximizes the between class distance [Fisher,\\n\\n\\n1936]. It can be efficiently computed in the linear case even with large data sets. How-\\n\\n\\never, often a linear separation is not sufficient. Nonlinear extensions by using kernels\\n\\n\\nexist [Mika et al., 2003], however, making it difficult to apply it to problems with large\\n\\n\\ntraining sets.\\n\\n\\nDecision Trees Another intuitive class of classification algorithms are decision trees.\\n\\n\\nThese algorithms solve the classification problem by repeatedly partitioning the in-\\n\\n\\n2\\n', metadata={'source': 's3://llm-showcase/papers/105-machine-learning-paper.pdf', 'page': 2}),\n",
       "  Document(page_content='put space, SO as to build a tree whose nodes\\n\\n\\nare as pure as possible (that is, they contain\\n\\n\\nDoes have\\n\\n\\nRoot\\n\\n\\nwarm blood?\\n\\n\\npoints of a single class). Classification of a\\n\\n\\nYes\\n\\n\\nNo\\n\\n\\nnew test point is achieved by moving from top\\n\\n\\nDoes it have\\n\\n\\nDo its adults\\n\\n\\nfeathers?\\n\\n\\nhave gills?\\n\\n\\nto bottom along the branches of the tree, start-\\n\\n\\nYes\\n\\n\\nNo\\n\\n\\nYes\\n\\n\\nNo\\n\\n\\ning from the root node, until a terminal node is\\n\\n\\nDoes it\\n\\n\\nBird\\n\\n\\nMammal\\n\\n\\nFish\\n\\n\\nreached. Decision trees are simple yet effec-\\n\\n\\nlive all its life\\n\\n\\non land?\\n\\n\\ntive classification schemes for small datasets.\\n\\n\\nYes\\n\\n\\nNo\\n\\n\\nThe computational complexity scales unfa-\\n\\n\\nReptile\\n\\n\\nAmphibian\\n\\n\\nvorably with the number of dimensions of the\\n\\n\\ndata. Large datasets tend to result in compli-\\n\\n\\nFigure 1: An example a decision tree (Figure\\n\\n\\ncated trees, which in turn require a large mem-\\n\\n\\ntaken from Yom-Tov [2004]).\\n\\n\\nory for storage. The C4.5 implementation by Quinlan [1992] is frequently used and\\n\\n\\ncan be downloaded at http / / www.rulequest.com/Personal\\n\\n\\nNeural Networks are perhaps one of the most commonly used approaches to clas-\\n\\n\\nsification. Neural networks (suggested first by Tur-\\n\\n\\ning [1992]) are a computational model inspired by\\n\\n\\nthe connectivity of neurons in animate nervous sys-\\n\\n\\nitems. A further boost to their popularity came with the\\n\\n\\nOutput\\n\\n\\nproof that they can approximate any function mapping\\n\\n\\nvia the Universal Approximation Theorem [Haykin,\\n\\n\\n1999]. A simple scheme for a neural network is\\n\\n\\nshown in Figure 2. Each circle denotes a computa-\\n\\n\\nInputs\\n\\n\\nHidden\\n\\n\\nOutput\\n\\n\\nlayer\\n\\n\\nlayer\\n\\n\\ntional element referred to as a neuron, which computes\\n\\n\\nFigure 2: A schematic diagram of a\\n\\n\\na weighted sum of its inputs, and possibly performs a\\n\\n\\nneural network. Each circle in the\\n\\n\\nnonlinear function on this sum. If certain classes of\\n\\n\\nhidden and output layer is a compu-\\n\\n\\nnonlinear functions are used, the function computed\\n\\n\\ntational element known as a neuron.\\n\\n\\nby the network can approximate any function (specif-\\n\\n\\n(Figure taken from Yom-Tov [2004])\\n\\n\\nically a mapping from the training patterns to the training targets), provided enough\\n\\n\\nneurons exist in the network and enough training examples are provided.\\n\\n\\n3.2 Large Margin Algorithms\\n\\n\\nMachine learning rests upon the theoretical foundation of Statistical Learning Theory\\n\\n\\n[e.g. Vapnik, 1995] which provides conditions and guarantees for good generalization\\n\\n\\nof learning algorithms. Within the last decade, large margin classification techniques\\n\\n\\nhave emerged as a practical result of the theory of generalization. Roughly speaking,\\n\\n\\nthe margin is the distance of the example to the separation boundary and a large mar-\\n\\n\\ngin classifier generates decision boundaries with large margins to almost all training\\n\\n\\nexamples. The two most widely studied classes of large margin classifiers are Support\\n\\n\\nVector Machines (SVMs) [Boser et al., 1992, Cortes and Vapnik, 1995] and Boosting\\n\\n\\n[Valiant, 1984, Schapire, 1992]:\\n\\n\\nSupport Vector Machines work by mapping the training data into a feature space\\n\\n\\nby the aid of a so-called kernel function and then separating the data using a large\\n\\n\\nmargin hyperplane (cf. Algorithm 1). Intuitively, the kernel computes a similarity\\n\\n\\n3\\n', metadata={'source': 's3://llm-showcase/papers/105-machine-learning-paper.pdf', 'page': 3}),\n",
       "  Document(page_content=\"between two given examples. Most commonly used kernel functions are RBF kernels\\n\\n\\n= and polynomial kernels k(x,x') = (x.x)d =\\n\\n\\nThe SVM finds a large margin separation between the training examples and pre-\\n\\n\\nviously unseen examples will often be close to the training examples. Hence, the large\\n\\n\\nmargin then ensures that these examples are correctly classified as well, i.e., the deci-\\n\\n\\nsion rule generalizes. For so-called positive definite kernels, the optimization problem\\n\\n\\ncan be solved efficiently and SVMs have an interpretation as a hyperplane separation\\n\\n\\nin a high dimensional feature space [Vapnik, 1995, Müller et al., 2001, Schölkopf and\\n\\n\\nSmola, 2002]. Support Vector Machines have been used on million dimensional data\\n\\n\\nsets and in other cases with more than a million examples [Mangasarian and Musi-\\n\\n\\ncant, 2001]. Research papers and implementations can be downloaded from the kernel\\n\\n\\nmachines web-site http //www.kernel-machines.org\\n\\n\\nAlgorithm 1 The Support Vector Machine with regularization parameter C.\\n\\n\\nGiven labeled sequences (X1,Y1), (Xm,Ym) (x X and Y E { [-1, +1}) and a\\n\\n\\nkernel k, the SVM computes a function\\n\\n\\nwhere the coefficients Ai and b are found by solving the optimization problem\\n\\n\\nminimize\\n\\n\\nsubject to\\n\\n\\nyif(xi)>1-\\n\\n\\nBoosting The basic idea of boosting and ensemble learning algorithms in general is\\n\\n\\nto iteratively combine relatively simple base hypotheses - sometimes called rules of\\n\\n\\nthumb - for the final prediction. One uses a so-called base learner that generates the\\n\\n\\nbase hypotheses. In boosting the base hypotheses are linearly combined. In the case\\n\\n\\nof two-class classification, the final prediction is the weighted majority of the votes.\\n\\n\\nThe combination of these simple rules can boost the performance drastically. It has\\n\\n\\nbeen shown that Boosting has strong ties to support vector machines and large margin\\n\\n\\nclassification [Rätsch, 2001, Meir and Rätsch, 2003]. Boosting techniques have been\\n\\n\\nused on very high dimensional data sets and can quite easily deal with than hundred\\n\\n\\nthousands of examples. Research papers and implementations can be downloaded\\n\\n\\nfrom tp://www.boosting.org\\n\\n\\n4 Summary\\n\\n\\nMachine Learning research has been extremely active the last few years. The result\\n\\n\\nis a large number of very accurate and efficient algorithms that are quite easy to use\\n\\n\\nfor a practitioner. It seems rewarding and almost mandatory for (computer) scientist\\n\\n\\nand engineers to learn how and where Machine Learning can help to automate tasks\\n\\n\\n4\\n\", metadata={'source': 's3://llm-showcase/papers/105-machine-learning-paper.pdf', 'page': 4}),\n",
       "  Document(page_content='or provide predictions where humans have difficulties to comprehend large amounts\\n\\n\\nof data. The long list of examples where Machine Learning techniques were success-\\n\\n\\nfully applied includes: Text classification and categorization [e.g. Joachims, 2001] (for\\n\\n\\ninstance spam filtering), network intrusion detection [e.g. Laskov et al., 2004], Bioin-\\n\\n\\nformatics (e.g. cancer tissue classification, gene finding; e.g. Furey et al. [2000], Zien\\n\\n\\net al. [2000], Sonnenburg et al. [2002]), brain computer interfacing [e.g. Blankertz\\n\\n\\net al., 2003], monitoring of electric appliances [e.g. Onoda et al., 2000], optimiza-\\n\\n\\ntion of hard disk caching strategies [e.g. Gramacy et al., 2003] and disk spin-down\\n\\n\\nprediction [e.g. Helmbold et al., 2000]), drug discovery [e.g. Warmuth et al., 2003]),\\n\\n\\nhigh-energy physics particle classification, recognition of hand writing, natural scene\\n\\n\\nanalysis etc.\\n\\n\\nObviously, in this brief summary I have to be far from being complete. I did not\\n\\n\\nmention regression algorithms (e.g. ridge regression, regression trees), unsupervised\\n\\n\\nlearning algorithms (such as clustering, principle component analysis), reinforcement\\n\\n\\nlearning, online learning algorithms or model-selection issues. Some of these tech-\\n\\n\\nniques extend the applicability of Machine Learning algorithms drastically and would\\n\\n\\neach require an introduction for them self. I would like to refer the interested reader\\n\\n\\nto two introductory books [Mendelson and Smola, 2003, von Luxburg et al., 2004]\\n\\n\\nwhich are the result of the last annual Summer Schools on Machine Learning (cf.\\n\\n\\nhttp://mlss.cc).\\n\\n\\nAcknowledgments I would like to thank Julia Lüning for encouraging me to write\\n\\n\\nthis paper. Furthermore, thanks to Sören Sonnenburg and Nora Toussaint for proof-\\n\\n\\nreading the manuscript. For the preparation of this work I used and modified some\\n\\n\\nparts of Müller et al. [2001], Rätsch [2001], Meir and Rätsch [2003], Yom-Tov [2004].\\n\\n\\nThis work was partly funded by the PASCAL Network of Excellence project (EU\\n\\n\\n#506778).\\n\\n\\nReferences\\n\\n\\nB.. Blankertz, G. Dornhege, C. Schäfer, R. Krepki, J. Kohlmorgen, K.-R. Müller, V. Kunz-\\n\\n\\nmann, F. Losch, and G. Curio. BCI bit rates and error detection for fast-pace motor com-\\n\\n\\nmands based on single-trial EEG analysis. IEEE Transactions on Neural Systems and Re-\\n\\n\\nhabilitation Engineering, 11:127-131, 2003.\\n\\n\\nB.E. Boser, I.M. Guyon, and V.N. Vapnik. A training algorithm for optimal margin classifiers.\\n\\n\\nIn D. Haussler, editor, Proceedings of the 5th Annual ACM Workshop on Computational\\n\\n\\nLearning Theory, pages 144-152, 1992.\\n\\n\\nC. Cortes and V.N. Vapnik. Support vector networks. Machine Learning, 20:273-297, 1995.\\n\\n\\nT.M. Cover and P.E. Hart. Nearest neighbor pattern classifications. IEEE transaction on\\n\\n\\ninformation theory, 13(1):21-27, 1967.\\n\\n\\nR.A. Fisher. The use of multiple measurements in taxonomic problems. Annals of Eugenics,\\n\\n\\n7:179-188, 1936.\\n\\n\\nT. Furey, N. Cristianini, N. Duffy, D. Bednarski, M. Schummer, and D. Haussler. Support\\n\\n\\nvector machine classification and validation of cancer tissue samples using microarray ex-\\n\\n\\npression data. Bioinformatics, 10:906-914, 2000.\\n\\n\\nR.B. Gramacy, M.K. Warmuth, S.A. Brandt, and I. Ari. Adaptive caching by refetching. In\\n\\n\\nIn Advances in Neural Information Processing Systems 15, pages 1465-1472. MIT Press,\\n\\n\\n2003.\\n\\n\\n5\\n', metadata={'source': 's3://llm-showcase/papers/105-machine-learning-paper.pdf', 'page': 5}),\n",
       "  Document(page_content=\"S. Haykin. Neural Networks: A comprehensive foundation, 2nd Ed. Prentice-Hall, 1999.\\n\\n\\nD.P. Helmbold, D.D.E. Long, T.L. Sconyers, and B. Sherrod. Adaptive disk spin-down for\\n\\n\\nmobile computers. Mobile Networks and Applications, 5(4):285-297, 2000.\\n\\n\\nT. Joachims. The Maximum-Margin Approach to Learning Text Classifiers. PhD thesis, Com-\\n\\n\\nputer Science Dept., University of Dortmund, 2001.\\n\\n\\nP. Laskov, C. Schäfer, and I. Kotenko. Intrusion detection in unlabeled data with quarter-sphere\\n\\n\\nsupport vector machines. In Proc. DIMVA, pages 71-82, 2004.\\n\\n\\nO.L. Mangasarian and D.R. Musicant. Lagrangian support vector machines. JMLR, 1:161-\\n\\n\\n177, March 2001.\\n\\n\\nR. Meir and G. Rätsch. An introduction to boosting and leveraging. In S. Mendelson\\n\\n\\nand A. Smola, editors, Advanced Lectures on Machine Learning, LNAI, pages 119-184.\\n\\n\\nSpringer, 2003.\\n\\n\\nS. Mendelson and A. Smola, editors. Advanced Lectures on Machine Learning, volume 2600\\n\\n\\nof LNAI. Springer, 2003.\\n\\n\\nS. Mika, G. Rätsch, J. Weston, B. Schölkopf, A.J. Smola, and K.-R. Müller. Constructing\\n\\n\\ndescriptive and discriminative nonlinear features: Rayleigh coefficients in kernel feature\\n\\n\\nspaces. IEEE Transactions on Patterns Analysis and Machine Intelligence, 25(5):623-627,\\n\\n\\nMay 2003.\\n\\n\\nK.-R. Müller, S. Mika, G. Rätsch, K. Tsuda, and B. Schölkopf. An introduction to kernel-based\\n\\n\\nlearning algorithms. IEEE Transactions on Neural Networks, 12(2):181-201, 2001.\\n\\n\\nT. Onoda, G. Rätsch, and K.-R. Müller. A non-intrusive monitoring system for household\\n\\n\\nelectric appliances with inverters. In H. Bothe and R. Rojas, editors, Proc. of NC'2000,\\n\\n\\nBerlin, 2000. ICSC Academic Press Canada/Switzerland.\\n\\n\\nJ.R. Quinlan. C4.5: Programs for Machine Learning. Morgan Kaufmann, 1992.\\n\\n\\nG. Rätsch. Robust Boosting via Convex Optimization. PhD thesis, University of Potsdam,\\n\\n\\nNeues Palais 10, 14469 Potsdam, Germany, October 2001.\\n\\n\\nR.E. Schapire. The Design and Analysis of Efficient Learning Algorithms. PhD thesis, MIT\\n\\n\\nPress, 1992.\\n\\n\\nB. Schölkopf and A.J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.\\n\\n\\nS. Sonnenburg, G. Rätsch, A. Jagota, and K.-R. Müller. New methods for splice-site recogni-\\n\\n\\ntion. In JR. Dorronsoro, editor, Proc. International conference on artificial Neural Networks\\n\\n\\n- ICANN'02, pages 329-336. LNCS 2415, Springer Berlin, 2002.\\n\\n\\nA.M. Turing. Intelligent machinery. In D.C. Ince, editor, Collected works of A.M. Turing:\\n\\n\\nMechanical Intelligence, Amsterdam, The Netherlands, 1992. Elsevier Science Publishers.\\n\\n\\nL.G. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134-1142,\\n\\n\\nNovember 1984.\\n\\n\\nV.N. Vapnik. The nature of statistical learning theory. Springer Verlag, New York, 1995.\\n\\n\\nU. von Luxburg, O. Bousquet, and G. Rätsch, editors. Advanced Lectures on Machine Learn-\\n\\n\\ning, volume 3176 of LNAI. Springer, 2004.\\n\\n\\nM. K Warmuth, J. Liao, G. Rätsch, Mathieson. M., S. Putta, and C. Lemmem. Support Vector\\n\\n\\nMachines for active learning in the drug discovery process. Journal of Chemical Information\\n\\n\\nSciences, 43(2):667-673, 2003.\\n\\n\\nW. Watanabe. Pattern recognition: Human and mechanical. Wiley, 1985.\\n\\n\\nE. Yom-Tov. An introduction to pattern classification. In U. von Luxburg, O. Bousquet, and\\n\\n\\nG. Rätsch, editors, Advanced Lectures on Machine Learning, volume 3176 of LNAI, pages\\n\\n\\n1-23. Springer, 2004.\\n\\n\\nA. Zien, G. Rätsch, S. Mika, B. Schölkopf, T. Lengauer, and K.-R. Müller. Engineering Sup-\\n\\n\\nport Vector Machine Kernels That Recognize Translation Initiation Sites. BioInformatics,\\n\\n\\n16(9):799-807, September 2000.\\n\\n\\n6\\n\", metadata={'source': 's3://llm-showcase/papers/105-machine-learning-paper.pdf', 'page': 6})],\n",
       " 'output_text': 'The given text provides an introduction to machine learning, focusing on supervised classification tasks. It defines machine learning as learning from examples to uncover patterns or make predictions on unseen data. The text discusses common classification algorithms like k-Nearest Neighbors, Linear Discriminant Analysis, Decision Trees, Naive Bayes, Support Vector Machines (SVMs), and Boosting. It highlights the importance of feature selection and pattern representation. The text then delves into the theoretical foundations of machine learning, including statistical learning theory and the Universal Approximation Theorem for neural networks. It emphasizes the widespread applicability of machine learning techniques across various domains, such as text classification, bioinformatics, and particle physics. The text concludes with a list of references related to machine learning, neural networks, and SVMs, spanning books, theses, and research papers from the early foundations to more recent developments.'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8dc6955f-e89a-458a-8a21-63d1a16ca013",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The given text provides an introduction to machine learning, focusing on supervised classification tasks. It defines machine learning as learning from examples to uncover patterns or make predictions on unseen data. The text discusses common classification algorithms like k-Nearest Neighbors, Linear Discriminant Analysis, Decision Trees, Naive Bayes, Support Vector Machines (SVMs), and Boosting. It highlights the importance of feature selection and pattern representation. The text then delves into the theoretical foundations of machine learning, including statistical learning theory and the Universal Approximation Theorem for neural networks. It emphasizes the widespread applicability of machine learning techniques across various domains, such as text classification, bioinformatics, and particle physics. The text concludes with a list of references related to machine learning, neural networks, and SVMs, spanning books, theses, and research papers from the early foundations to more recent developments.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['output_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "971442fc-573b-4296-8f55-c0e568e45e17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_text': 'The given text provides an introduction to machine learning, focusing on supervised classification tasks. It defines machine learning as learning from examples to uncover patterns or make predictions on unseen data. The text discusses common classification algorithms like k-Nearest Neighbors, Linear Discriminant Analysis, Decision Trees, Naive Bayes, Support Vector Machines (SVMs), and Boosting. It highlights the importance of feature selection and pattern representation. The text then delves into the theoretical foundations of machine learning, including statistical learning theory and the Universal Approximation Theorem for neural networks. It emphasizes the widespread applicability of machine learning techniques across various domains, such as text classification, bioinformatics, and particle physics. The text concludes with a list of references related to machine learning, neural networks, and SVMs, spanning books, theses, and research papers from the early foundations to more recent developments.',\n",
       " 'text': '<thinking>\\nCLUES:\\n- The text provides an introduction to machine learning, focusing on supervised classification tasks. It covers various classification algorithms like k-Nearest Neighbors, Linear Discriminant Analysis, Decision Trees, Naive Bayes, Support Vector Machines, and Boosting.\\n- It discusses the importance of feature selection and pattern representation in machine learning.\\n- The text also covers theoretical foundations like statistical learning theory and the Universal Approximation Theorem for neural networks.\\n- It highlights the applicability of machine learning techniques across various domains, including text classification, bioinformatics, and particle physics.\\n- Based on the content covering a broad overview of machine learning concepts, algorithms, and theory, this paper seems to be related to the General ML field.\\n</thinking>\\n\\n<label>General ML</label>'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bedrock = boto3.client(service_name='bedrock-runtime')\n",
    "bedrock_llm = BedrockChat(client=bedrock, model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\")\n",
    "\n",
    "llm_chain = LLMChain(prompt=mmr_prompt, llm=bedrock_llm)\n",
    "\n",
    "llm_response = llm_chain.invoke(output['output_text'])\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4c0599-d62d-4238-b499-fd4324105a45",
   "metadata": {},
   "source": [
    "## Prompt engineering (Extract Key phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd62933c-3cff-4855-acb1-1a5c9f15f252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_keypoints = \"\"\" \n",
    "\n",
    "<instructions>\n",
    "You are a Scientific paper system. \n",
    "Please do the following:\n",
    "1. Extract the authors from the paper and add them into <author></author> tags.\n",
    "2. Extract the paper's publish date and add it into <date></date> tags.\n",
    "3. Extract the paper's title and add it into <title></title> tags.\n",
    "</instructions>\n",
    "\n",
    "<document>{doc_text}</document>\n",
    "\n",
    "<author></author>\n",
    "<date></date>\n",
    "<title></title>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "65fd32b1-4932-4b95-be6a-380b12d6c3d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_text': 'A Brief Introduction into Machine Learning\\n\\n\\nGunnar Rätsch\\n\\n\\nFriedrich Miescher Laboratory of the Max Planck Society,\\n\\n\\nSpemannstraße 37, 72076 Tübingen, Germany\\n\\n\\nttp://www.tuebingen.mpg.de/-raetsch\\n\\n\\n1 Introduction\\n\\n\\nThe Machine Learning field evolved from the broad field of Artificial Intelligence,\\n\\n\\nwhich aims to mimic intelligent abilities of humans by machines. In the field of Ma-\\n\\n\\nchine Learning one considers the important question of how to make machines able\\n\\n\\nto \"learn\". Learning in this context is understood as inductive inference, where one\\n\\n\\nobserves examples that represent incomplete information about some \"statistical phe-\\n\\n\\nnomenon\". In unsupervised learning one typically tries to uncover hidden regularities\\n\\n\\n(e.g. clusters) or to detect anomalies in the data (for instance some unusual machine\\n\\n\\nfunction or a network intrusion). In supervised learning, there is a label associated\\n\\n\\nwith each example. It is supposed to be the answer to a question about the example. If\\n\\n\\nthe label is discrete, then the task is called classification problem - otherwise, for real-\\n\\n\\nvalued labels we speak of a regression problem. Based on these examples (including\\n\\n\\nthe labels), one is particularly interested to predict the answer for other cases before\\n\\n\\nthey are explicitly observed. Hence, learning is not only a question of remembering\\n\\n\\nbut also of generalization to unseen cases.\\n\\n\\n2 Supervised Classification\\n\\n\\nAn important task in Machine Learning is classification, also referred to as pattern\\n\\n\\nrecognition, where one attempts to build algorithms capable of automatically con-\\n\\n\\nstructing methods for distinguishing between different exemplars, based on their dif-\\n\\n\\nferentiating patterns.\\n\\n\\nWatanabe [1985] described a pattern as \"the opposite of chaos; it is an entity,\\n\\n\\nvaguely defined, that could be given a name.\" Examples of patterns are human faces,\\n\\n\\ntext documents, handwritten letters or digits, EEG signals, and the DNA sequences that\\n\\n\\nmay cause a certain disease. More formally, the goal of a (supervised) classification\\n\\n\\ntask is to find a functional mapping between the input data X, describing the input\\n\\n\\npattern, to a class label Y (e.g. or +1), such that Y = f(X). The construction of\\n\\n\\nthe mapping is based on so-called training data supplied to the classification algorithm.\\n\\n\\nThe aim is to accurately predict the correct label on unseen data.\\n\\n\\nA pattern (also: \"example\") is described by its features. These are the characteris-\\n\\n\\ntics of the examples for a given problem. For instance, in a face recognition task some\\n\\n\\nfeatures could be the color of the eyes or the distance between the eyes. Thus, the input\\n\\n\\n1\\n',\n",
       " 'text': '<author>Gunnar Rätsch</author>\\n<date></date>\\n<title>A Brief Introduction into Machine Learning</title>'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_key = PromptTemplate(template=prompt_keypoints, input_variables=[\"doc_text\"])\n",
    "bedrock_llm = BedrockChat(client=bedrock, model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\")\n",
    "llm_chain = LLMChain(prompt=prompt_key, llm=bedrock_llm)\n",
    "llm_response = llm_chain.invoke(document[0].page_content)\n",
    "llm_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4942b-58d4-4193-abc2-aee8aa45426d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
